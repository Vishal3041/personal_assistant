{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10143260,"sourceType":"datasetVersion","datasetId":6260772}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pinecone sentence_transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:15:46.737678Z","iopub.execute_input":"2024-12-09T02:15:46.738578Z","iopub.status.idle":"2024-12-09T02:15:59.727288Z","shell.execute_reply.started":"2024-12-09T02:15:46.738536Z","shell.execute_reply":"2024-12-09T02:15:59.726338Z"}},"outputs":[{"name":"stdout","text":"Collecting pinecone\n  Downloading pinecone-5.4.1-py3-none-any.whl.metadata (19 kB)\nCollecting sentence_transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: certifi>=2019.11.17 in /opt/conda/lib/python3.10/site-packages (from pinecone) (2024.6.2)\nCollecting pinecone-plugin-inference<4.0.0,>=2.0.0 (from pinecone)\n  Downloading pinecone_plugin_inference-3.0.0-py3-none-any.whl.metadata (2.2 kB)\nCollecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from pinecone) (2.9.0.post0)\nRequirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from pinecone) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.10/site-packages (from pinecone) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from pinecone) (1.26.18)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.46.3)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.26.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone) (1.16.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading pinecone-5.4.1-py3-none-any.whl (427 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pinecone_plugin_inference-3.0.0-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\nInstalling collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone, sentence_transformers\nSuccessfully installed pinecone-5.4.1 pinecone-plugin-inference-3.0.0 pinecone-plugin-interface-0.0.7 sentence_transformers-3.3.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import logging\nlogging.basicConfig(level=logging.DEBUG)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:34:56.691142Z","iopub.execute_input":"2024-12-09T00:34:56.691654Z","iopub.status.idle":"2024-12-09T00:34:56.697223Z","shell.execute_reply.started":"2024-12-09T00:34:56.691611Z","shell.execute_reply":"2024-12-09T00:34:56.696011Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"!pip install --upgrade pinecone-client","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:16:33.969337Z","iopub.execute_input":"2024-12-09T02:16:33.969728Z","iopub.status.idle":"2024-12-09T02:16:43.660701Z","shell.execute_reply.started":"2024-12-09T02:16:33.969695Z","shell.execute_reply":"2024-12-09T02:16:43.659782Z"}},"outputs":[{"name":"stdout","text":"Collecting pinecone-client\n  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: certifi>=2019.11.17 in /opt/conda/lib/python3.10/site-packages (from pinecone-client) (2024.6.2)\nCollecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /opt/conda/lib/python3.10/site-packages (from pinecone-client) (0.0.7)\nRequirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from pinecone-client) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.10/site-packages (from pinecone-client) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from pinecone-client) (1.26.18)\nDownloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pinecone-plugin-inference, pinecone-client\n  Attempting uninstall: pinecone-plugin-inference\n    Found existing installation: pinecone-plugin-inference 3.0.0\n    Uninstalling pinecone-plugin-inference-3.0.0:\n      Successfully uninstalled pinecone-plugin-inference-3.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npinecone 5.4.1 requires pinecone-plugin-inference<4.0.0,>=2.0.0, but you have pinecone-plugin-inference 1.1.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!ls /kaggle/input/transnormerllm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:17:13.733085Z","iopub.execute_input":"2024-12-09T02:17:13.733444Z","iopub.status.idle":"2024-12-09T02:17:14.836651Z","shell.execute_reply.started":"2024-12-09T02:17:13.733411Z","shell.execute_reply":"2024-12-09T02:17:14.835387Z"}},"outputs":[{"name":"stdout","text":"config.json\t\tspecial_tokens_map.json   tokenizer_config.json\ngeneration_config.json\ttokenization_baichuan.py\npytorch_model.bin\ttokenizer.model\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install einops triton","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:17:19.829659Z","iopub.execute_input":"2024-12-09T02:17:19.830439Z","iopub.status.idle":"2024-12-09T02:17:39.242916Z","shell.execute_reply.started":"2024-12-09T02:17:19.830401Z","shell.execute_reply":"2024-12-09T02:17:39.241406Z"}},"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting triton\n  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton) (3.15.1)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, einops\nSuccessfully installed einops-0.8.0 triton-3.1.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# Define the model path\nmodel_path = \"/kaggle/input/transnormerllm\"\n\n# Load the model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:17:50.089932Z","iopub.execute_input":"2024-12-09T02:17:50.090291Z","iopub.status.idle":"2024-12-09T02:18:16.019021Z","shell.execute_reply.started":"2024-12-09T02:17:50.090261Z","shell.execute_reply":"2024-12-09T02:18:16.018166Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"configuration_transnormer.py:   0%|          | 0.00/2.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f2d58220082422faa04264137ce2544"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/OpenNLPLab/TransNormerLLM-1B:\n- configuration_transnormer.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_transnormer.py:   0%|          | 0.00/34.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8ac41cd64da4b5084c96bbeb30af54e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"utils.py:   0%|          | 0.00/3.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b74317527059465a931029d3d0662708"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"srmsnorm_triton.py:   0%|          | 0.00/5.75k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6776b5b3c2848f3984e0c84440cb8b0"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/OpenNLPLab/TransNormerLLM-1B:\n- srmsnorm_triton.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"norm.py:   0%|          | 0.00/1.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"121bb2317c1448e0b5c413eb83b683dd"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/OpenNLPLab/TransNormerLLM-1B:\n- norm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/OpenNLPLab/TransNormerLLM-1B:\n- utils.py\n- srmsnorm_triton.py\n- norm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"lightning_attention2.py:   0%|          | 0.00/15.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00392684604f4afbb471588e4190802a"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/OpenNLPLab/TransNormerLLM-1B:\n- lightning_attention2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/OpenNLPLab/TransNormerLLM-1B:\n- modeling_transnormer.py\n- utils.py\n- lightning_attention2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Check if GPU is available and set device accordingly\ndevice = 0 if torch.cuda.is_available() else -1\n\nrag_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    truncation=True,\n    device=device\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:18:37.910044Z","iopub.execute_input":"2024-12-09T02:18:37.910713Z","iopub.status.idle":"2024-12-09T02:19:11.126411Z","shell.execute_reply.started":"2024-12-09T02:18:37.910663Z","shell.execute_reply":"2024-12-09T02:19:11.125618Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())  # Should return True\nprint(torch.cuda.get_device_name(0))  # Should return the GPU name if available","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:15:20.018354Z","iopub.execute_input":"2024-12-09T02:15:20.018715Z","iopub.status.idle":"2024-12-09T02:15:22.217062Z","shell.execute_reply.started":"2024-12-09T02:15:20.018667Z","shell.execute_reply":"2024-12-09T02:15:22.216034Z"}},"outputs":[{"name":"stdout","text":"True\nTesla T4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Test the pipeline\noutput = rag_pipeline(\"Once upon a time\", max_length=50, num_return_sequences=1)\nprint(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:19:17.260767Z","iopub.execute_input":"2024-12-09T02:19:17.261161Z","iopub.status.idle":"2024-12-09T02:19:24.927356Z","shell.execute_reply.started":"2024-12-09T02:19:17.261127Z","shell.execute_reply":"2024-12-09T02:19:24.926623Z"}},"outputs":[{"name":"stderr","text":"Both `max_new_tokens` (=2048) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"[{'generated_text': 'Once upon a time, before the fall, the Earth had several primary continents. The largest of these was Africa, being the largest landmass and being home to most of its inhabitants. Africa had many small island nations as well, and it has been described as the cradle of mankind. The continent was primarily inhabited by a multitude of native peoples, ranging in size from the Congo to South Sudan, some of whom are now part of Africa as states today.\\nQuestion: did africa have a continents?\\nAnswer: True'}]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from pinecone import Pinecone\nfrom sentence_transformers import SentenceTransformer\n\n# Initialize Pinecone client\npc = Pinecone(\n    api_key=\"pcsk_5nP68u_U7MeWfAo2hKpwwkmWLyAC2VNEuCgZo8EQzs4Zox7yyrdUAXjM5rttShTEEGAPuZ\"\n)\n\n# Define the index name\nindex_name = \"chrome-history-index\"\n\n# Check if the index exists\nexisting_indexes = pc.list_indexes().names()  # Get list of existing indexes\nif index_name not in existing_indexes:\n    raise ValueError(f\"Index '{index_name}' does not exist. Please verify the name or create it in the Pinecone dashboard.\")\n\n# Access the Pinecone index\nindex = pc.Index(index_name)\n\n# Load embedding model\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:19:34.198662Z","iopub.execute_input":"2024-12-09T02:19:34.199110Z","iopub.status.idle":"2024-12-09T02:19:40.521348Z","shell.execute_reply.started":"2024-12-09T02:19:34.199076Z","shell.execute_reply":"2024-12-09T02:19:40.520561Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20ae13b8c5f54dd193f11ffe63f05373"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a7c654a0b564e84ab93ed39d92975e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25b9fde780094fd3bf4b8e72ccde5ae2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eaa1e690eb54ce4b524a5e719eb2f73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbd80a1861874cf78c36ba92e819dde7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f73afc8518424cf7a6b8dd26e166032a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aed1efd29a2426eac85e9c14513a854"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"519e9d0272a64895b51ec44601a49f5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4b214d841044bcf95467bce334f9feb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4551fb4f0d464ffc8463871a8395c628"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8845c4222f8441bdba4d92766970367d"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def perform_filtered_query(query, filter_conditions=None, query_vector=None):\n    \"\"\"\n    Perform a Pinecone query with combined filters and augment the result for Chrome history.\n    \"\"\"\n    if filter_conditions is None:\n        filter_conditions = {}\n\n    # Use query_vector for title-based search, or default vector for filter-only queries\n    if query_vector is None:\n        query_vector = [0.0] * 384  # Dummy vector for metadata-only filtering\n\n    # Query Pinecone with combined filters\n    results = index.query(\n        vector=query_vector,\n        top_k=5,\n        include_metadata=True,\n        filter=filter_conditions\n    )\n\n    # Construct context with titles and timestamps, filtered by domain or time\n    context = \"\\n\".join([\n        f\"{res['metadata']['Title']} ({res['metadata']['Timestamp']})\"\n        for res in results[\"matches\"]\n    ])\n\n    # Limit context length to avoid exceeding model limits\n    max_context_length = 300  # Adjust based on model's input size\n    truncated_context = context[:max_context_length]\n\n    # Augment the query with the context\n    augmented_query = f\"Here are some results for your query:\\n{truncated_context}\\n\\nQuery: {query}\"\n    \n    # Debug: Print the augmented query\n    print(f\"Augmented Query:\\n{augmented_query}\\n\")\n\n    # Generate response\n    response = rag_pipeline(augmented_query, max_new_tokens=100, num_return_sequences=1)\n    \n    # Handle empty or nonsensical responses\n    if response and response[0][\"generated_text\"].strip():\n        return response[0][\"generated_text\"]\n    else:\n        return \"No relevant entries found for the specified query or time range.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:26:52.292193Z","iopub.execute_input":"2024-12-09T02:26:52.292526Z","iopub.status.idle":"2024-12-09T02:26:52.299728Z","shell.execute_reply.started":"2024-12-09T02:26:52.292499Z","shell.execute_reply":"2024-12-09T02:26:52.298763Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"import re\n\ndef perform_rag_query(query):\n    \"\"\"\n    Perform a RAG query by detecting query type and routing to the appropriate handler.\n    Supports combined queries (date + domain + title).\n    \"\"\"\n    filter_conditions = {}\n    query_vector = None\n\n    # Detect date in query\n    date_match = re.search(r'\\b(\\d{4}-\\d{2}-\\d{2})\\b', query, re.IGNORECASE)\n    if date_match:\n        filter_conditions[\"Timestamp\"] = date_match.group(1)\n    \n    # Detect domain in query\n    domain_match = re.search(r'\\b(?:www\\.)?([a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})\\b', query, re.IGNORECASE)\n    if domain_match:\n        filter_conditions[\"Domain\"] = domain_match.group(1)\n    \n    # Detect title keywords (if no explicit date/domain filter)\n    if len(filter_conditions) == 0 or \"title\" in query.lower():\n        query_vector = embedding_model.encode(query).tolist()\n\n    # Pass combined filters and query vector to `perform_filtered_query`\n    return perform_filtered_query(query, filter_conditions=filter_conditions, query_vector=query_vector)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:26:53.690436Z","iopub.execute_input":"2024-12-09T02:26:53.691229Z","iopub.status.idle":"2024-12-09T02:26:53.696802Z","shell.execute_reply.started":"2024-12-09T02:26:53.691195Z","shell.execute_reply":"2024-12-09T02:26:53.695941Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Example usage\nquery = \"List all the movies i watched of govinda\"\nresponse = perform_rag_query(query)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:26:54.501256Z","iopub.execute_input":"2024-12-09T02:26:54.502088Z","iopub.status.idle":"2024-12-09T02:26:56.683666Z","shell.execute_reply.started":"2024-12-09T02:26:54.502054Z","shell.execute_reply":"2024-12-09T02:26:56.682636Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3bfa0067b2e44c5b6aacb910d0af6fc"}},"metadata":{}},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"Augmented Query:\nHere are some results for your query:\n(4) Coolie No. 1 | All Comedy Scenes | Govinda | Karishma Kapoor | Pooja Films - YouTube (2024-04-05 20:21:09)\ngovinda hululu - Google Search (2024-07-27 05:38:55)\ngovinda hululu - Google Search (2024-07-27 05:38:41)\n(5) Hungama - Part 7 | Paresh Rawal, Rajpal Yadav & Akshaye Khanna | Hindi Movies |\n\nQuery: List all the movies i watched of govinda\n\nHere are some results for your query:\n(4) Coolie No. 1 | All Comedy Scenes | Govinda | Karishma Kapoor | Pooja Films - YouTube (2024-04-05 20:21:09)\ngovinda hululu - Google Search (2024-07-27 05:38:55)\ngovinda hululu - Google Search (2024-07-27 05:38:41)\n(5) Hungama - Part 7 | Paresh Rawal, Rajpal Yadav & Akshaye Khanna | Hindi Movies |\n\nQuery: List all the movies i watched of govinda kumar ji, and their directors from 1985 to 1989.\n\nAnswer: There are several movies that govinda kumar ji starred in, from 1985 to 1989. You can use the keywords as provided in the given options and select the movies you wish to watch.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}